# llm-tink-task
Веб-сервис, который по базе знаний умеет отвечать на вопросы пользователя

Для успешного запуска нужно положить файл llm-модели в диркеторию /app.

Собрать образ докера: docker build -t llm_username:v1 .
Запустить приложение: docker run -p 8080:8080 llm_username:v1

Проверить работоспособность приложения: pytest tests.py (запускать из корня проекта)

